{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Language of Taste. Discursive Aspects of Brussels' Restaurant Reviews, 1960 - 2000. Data and code for chapter 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 2 describes the theory and methodology for this thesis. As such, it contains a significant amount of information about corpus design and considerations in corpus building. In this Jupyter Notebook, the steps of putting together the corpora used in this thesis are explained in greater detail, together with the code used to clean, enrich and query the data. Additionally, this notebook will be made available, together with the underlying data, upon submission of the associated thesis. Doing so provides added value in three distinct ways: 1) it provides transparency in how the results were obtained and as such can be considered a safeguard against against scientific fraud, 2) by making both data and code available, experiments can be repeated and adapted by anyone. While sharing the data fits within the framework of Open Data, it is its openness to be adapted that holds promise for future research. On the one hand, the underlying data can be queried along different analytical dimensions than those used in this text. The code which was written to analyse the texts, on the other hand, can easily be modified to run similar analyses on different texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framework used for building and querying the corpora is NLTK (Natural Language Toolkit) for Python. NLTK is an open-source library which contains a rich set of tools for building and interacting with corpora. These corpora can be premade (these are included in the NLTK download) or they can be created by the user. The latter technique was employed for this thesis and will be explained in the notebook which accompanies the next chapter. For the explanation of methods and techniques, some built-in corpora are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a corpus is loaded into memory, it can be queries and examined. There are different types of corpora. The corpus loaded here as an example is the Brown Corpus, one of the oldest digital English text collections. It is a categorized corpus which contains samples from different text genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownGenres = brown.categories()\n",
    "print(f'The Brown Corpus has {len(brownGenres)} categories')\n",
    "print(brownGenres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest form, a corpus is a more or less structured collection of texts. This is also the case for the Brown corpus and is also the case for the corpora used in this thesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, these individual files can be examined. Although it is rare that an analysis focuses on a single text in a query, it is quite common to use a single text to test code samples and for rapid development. By restricting the analysis on a single text, complexity is reduced, whcih allows for faster development and testing. Lets take a closer look at 'ca01':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsca01 = brown.words(['ca01'])\n",
    "print(wordsca01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ellipsis shows that the full wordlist is loaded into memory, but only part is displayed. It is possible to change the selection of tokens by using Python slice syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsca01_50 = brown.words(['ca01'])[:50]\n",
    "wordsca01_last50 = brown.words(['ca01'])[-50:]\n",
    "print(wordsca01_50)\n",
    "print(wordsca01_last50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to display the complete list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in wordsca01:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting words does not have to be based on a single file, it can also be done across a category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_text = brown.words(categories='adventure')\n",
    "print(adv_text)\n",
    "print(f'The adventure category contains {len(adv_text)} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bird, Klein and Loper (2014) show in one of their examples how much insight can be gained through a minimal amount of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(f'{m}: {fdist[m]}', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing frequencies and tabulating the results allows for quick comparisons across genres or periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
